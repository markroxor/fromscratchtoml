{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised_Learning\n",
    "## Linear Regression\n",
    "Let's say we have a dataset of type ```[(x_1, y_1), (x_2, y_2), (x_3, y_3) ... (x_n, y_n)]``` where xi = size of house and yi = price of house in a particular area.\n",
    "\n",
    "The task is train your system on the given dataset and predict the price for on an unseen size. \n",
    "\n",
    "We can train a hypothesis function using linear regression which can predict the price for a given size. The hypothesis function for linear regression can be can be derived in two ways. The first way is trivial but interesting which is known as the analytical approach or ordinary least square. The following section will describe about the analytical approach.\n",
    "\n",
    "##### Analytical Approach\n",
    "y = target variable\n",
    "\n",
    "x = predictor variable\n",
    "\n",
    "take a hypothesis function ```f(x) = b_1x+b_0``` ( which is the equation of a straight line, 12th standard math).\n",
    "b0 and b1 are the parameters where b1 = slope of the line and b0 = intercept. If you are able to find these two parameters then you have found the hypothesis function. The analytical approach can compute the parameter b0 and b1 directly from the training data.\n",
    "\n",
    "#### Let's see the couple of mathematical equations.\n",
    "In machine learning literature, we find the hypothesis function by computing the loss function. We try to optimize the loss function in a way that the given the input the loss should be the minimum. This loss function is also known as cost function or objective function.\n",
    "\n",
    "There exist varieties of loss function. However, we will use the simplest loss function (L2 loss). The equations below describes the Loss function and the steps to compute the first parameter b0.\n",
    "\n",
    "$$\n",
    "L =\\frac{1}{2n} \\sum\\limits_{i = 1}^n [{{y _i - f(x_i)}}]^2\n",
    "$$\n",
    "\n",
    "Now substitute the value of f(x) in the above equation.\n",
    "\n",
    "$$\n",
    "L =\\frac{1}{2n} \\sum\\limits_{i = 1}^n [{{y _i - b_1x_i -b_0}}]^2\n",
    "$$\n",
    "\n",
    "Differentiate the above equation with respect to `b_1` and ```b_0```. After differentiation equate it to zero to get the optimal value of ```b_1``` and ```b_0```.\n",
    "\n",
    "$$\n",
    "\\frac{\\delta L}{\\delta b_0} = \\frac{-1}{n}\\sum\\limits_{i = 1}^n[y_i -b_1x_i -b_0]\n",
    "$$\n",
    "\n",
    "$$\n",
    "0 = \\frac{-1}{n}\\sum\\limits_{i = 1}^n[y_i -b_1x_i -b_0]\n",
    "$$\n",
    "\n",
    "Apply summation to each component\n",
    "\n",
    "$$ \n",
    "0 = -\\frac{1}{n}\\sum\\limits_{i = 1}^n [y_i] + \\frac{1}{n}\\sum\\limits_{i = 1}^n [b_1x_i] + \\frac{1}{n}\\sum\\limits_{i = 1}^n [b_0]\n",
    "$$\n",
    "\n",
    "Finally, the b_0 becomes\n",
    "\n",
    "$$ \n",
    "\\frac{1}{n}\\sum\\limits_{i = 1}^n [y_i] - \\frac{1}{n}\\sum\\limits_{i = 1}^n [b_1x_i] = \\frac{1}{n}\\sum\\limits_{i = 1}^n [b_0]\n",
    "$$\n",
    "\n",
    "i.e.,\n",
    "\n",
    "$$\n",
    "b_0 = \\bar y - b_1\\bar x\n",
    "$$\n",
    "\n",
    "As we can see in the final above equation that the value of b0 = mean(y) - b1*mean(x). This implies that the first step in analaytical approach is to compute the mean for target and predictor.\n",
    "\n",
    "Substitute the value of b_0 in the top most equation for the loss function. \n",
    "\n",
    "$$ \n",
    "L  =\\frac{1}{2n} \\sum\\limits_{i = 1}^n [{{y _i - b_1x_i -\\bar y + b_1\\bar x}}]^2\n",
    "$$\n",
    "\n",
    "Now differentiate the above equation\n",
    "\n",
    "$$\n",
    "\\frac{\\delta L}{\\delta b_1} = \\frac{1}{n}\\sum\\limits_{i = 1}^n[{{(y _i -\\bar y) - (b_1\\bar x -b_1x_i) }}][\\bar x - x_i]\n",
    "$$\n",
    "\n",
    "Equating left hand side to zero and solving for b_1, we will get the following.\n",
    "\n",
    "$$\n",
    "b_1 = \\frac{\\sum\\limits_{i = 1}^n (y_i - \\bar y)(x_i - \\bar x)}{\\sum\\limits_{i = 1}^n (x_i - \\bar x)^2 }\n",
    "$$\n",
    "\n",
    "\n",
    "As shown above in the equation that the parameter b_1 is the ratio of covariance of x,y to variance of x. Therefore, the second step is to calculate the cov(x,y) and var(x). Use these two values to compute the parameter b_1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, train_x, train_y, optimizer):\n",
    "        \"\"\" Finds suitable value of parameters which best fit the given output values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_x : list of (int/float torch.Tensor).\n",
    "                  The input values on which linear regression is performed.\n",
    "        train_y : list of (int/float torch.Tensor).\n",
    "                  The target labels corresponding to the input values.\n",
    "        optimizers : analytical\n",
    "                     Trains parameters to minimize the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        if optimizer == 'analytical':\n",
    "            cov_x_y = []\n",
    "            var_x = []\n",
    "\n",
    "            mean_x = ch.mean(train_x)\n",
    "            mean_y = ch.mean(train_y)\n",
    "\n",
    "            for x, y in zip(train_x, train_y):\n",
    "                cov_x_y.append((x - mean_x) * (y - mean_y))\n",
    "                var_x.append((x - mean_x) ** 2)\n",
    "\n",
    "            self.beta = sum(cov_x_y) / sum(var_x)\n",
    "            self.alpha = mean_y - self.beta * mean_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
