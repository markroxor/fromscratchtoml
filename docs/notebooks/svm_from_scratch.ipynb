{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (from scratch)\n",
    "\n",
    "Support vector machines are supervised machine learning models which can address the problems of both classification and regression.\n",
    "\n",
    "Find more tutorials like this at [fromscratchtoml](https://github.com/jellAIfish/fromscratchtoml)  \n",
    "Visit the code for this implementation [here](https://github.com/jellAIfish/fromscratchtoml/blob/master/fromscratchtoml/models/svm/svc.py)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT VECTOR CLASSIFIERS\n",
    "\n",
    "<img src=\"https://github.com/jellAIfish/fromscratchtoml/raw/svc_nb/docs/project/static/notebooks/images/1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "k = \\frac{w}{|w|}|k| \\\\\n",
    "x = k + x' \\\\\n",
    "x' = x - k \\\\\n",
    " \\end{array}\n",
    "$$\n",
    "\n",
    "our hypothesis equation -\n",
    "$$ f(x) = w.x + b $$\n",
    "Since the point `x'` lies on our hyperplane -\n",
    "$$ w.x' + b = 0 $$\n",
    "substituting `x'`\n",
    "$$ \\begin{array}{ll}\n",
    "w.(x-k) + b = 0 \\\\\n",
    "w.(x-\\frac{w}{|w|}|k|)+b = 0 \\\\\n",
    "w.x - |w|k + b = 0 \\\\\n",
    "k = \\frac{w.x + b}{|w|} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "If `k` is positive the point is at one side of the hyperplane and vica-versa but since we are concerned only about the magnitude we multiply `k` with the corresponding label `y`,\n",
    "\n",
    "$$\\gamma = y(\\frac{w.x + b}{|w|})$$\n",
    "\n",
    "This is called geomatric margin which is the euclidean distance between `x` and our hyperlane. It will always be positive since  $y \\in [1, -1]$.\n",
    "\n",
    "Our goal is to maximise the minimum geomatric margin.\n",
    " Mathematically we want to maximise $\\gamma$  w.r.t `w` and `b`-\n",
    "\n",
    "$$\\mathop {\\max }\\limits_{w,b} \\gamma$$\n",
    "\n",
    "$$\\gamma \\le y_i(\\frac{w.x_i + b}{|w|}) ,i \\in [1, m]$$\n",
    "such that it is the minimum geomatric margin amongst all the margins.\n",
    "\n",
    "Notice that our hypothesis function\n",
    "$$ f(x) = w.x + b $$\n",
    "wont change if we scale it up by some positive factor `t`\n",
    "$$ f_{new}(x) = t(w.x + b) $$\n",
    "\n",
    "Since for classifying purpose we only require the magnitude of our hypothesis function. This means that for any `x` we can scale $f(x)$ such that $f(x) = 1$.\n",
    "\n",
    "\n",
    "Therefore our min-max equation becomes.\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{maximize} \\frac{1}{|w|}\\\\\n",
    "\\mbox{subject to } 1 \\le y_i(\\frac{w.x_i + b}{|w|}) ,i \\in [1, m]\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Also maximising $\\frac{1}{|w|}$ will lead to the same result if we maximise $\\frac{|w|^2}{2}$. We do this because $\\frac{1}{|w|}$ is a non convex function.  \n",
    "<img src=\"https://github.com/jellAIfish/fromscratchtoml/raw/svc_nb/docs/project/static/notebooks/images/3.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "Our goal is to minimise\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{minimise} \\frac{|w|^2}{2}\\\\\n",
    "\\mbox{subject to } \\le y_i(\\frac{w.x_i + b}{|w|}) - 1 ,i \\in [1, m]\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Lagrange equations\n",
    "If we want to minimise $f(x)$\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mbox{minimise  } f(x)\\\\\n",
    "\\mbox{subject to } g(x) = 0\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "then the minimum is found when the derivatives of both the functions point in the same direction.\n",
    "$$\\nabla f(x) = \\alpha \\nabla g(x)$$\n",
    "$$\\nabla f(x) - \\alpha \\nabla g(x) = 0$$\n",
    "if we define a lagrange function such that -\n",
    "$$L(x,\\alpha ) = f(x) - \\alpha g(x)$$\n",
    "then -\n",
    "$${\\nabla}L(x,\\alpha ) = \\nabla f(x) - \\alpha \\nabla g(x)$$\n",
    "\n",
    "$$ L(x, \\alpha) = f(x) - \\sum\\limits_{i=1}^m\\alpha g(x)$$  \n",
    "\n",
    "where $\\alpha$ is lagrange multiplier.\n",
    "\n",
    "Using this for our minimisation problem.\n",
    "$$ L(w, b, \\alpha) = \\frac{|w|^2}{2} - \\sum\\limits_{i=1}^m\\alpha_i [ y_i(\\frac{w.x_i + b}{|w|}) - 1]$$  \n",
    "\n",
    "It's dual form is\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\mathop {\\max }\\limits_\\alpha \\mathop {\\min }\\limits_{w,b}L(w,b,\\alpha )\\\\\n",
    "\\mbox{subject to } \\alpha_i \\ge 0, i \\in [1, m]\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To minimise wrt to `w` and `b` -\n",
    "$${\\nabla_w}L = w - \\sum\\limits_{i=1}^m\\alpha_i y_i x_i = 0$$\n",
    "$${\\nabla_b}L = \\sum\\limits_{i=1}^m\\alpha_i y_i = 0$$\n",
    "\n",
    "substituting these values back in our lagrange equation.\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "L(\\alpha) = \\frac{|w|^2}{2} - \\sum\\limits_{j=1}^m\\alpha_j [ y_j(w.x_i + b) - 1]\\\\\n",
    "L(\\alpha) = \\frac{\\sum\\limits_{i=1}^m\\sum\\limits_{i=j}^m\\alpha_i y_i x_i\\alpha_j y_j x_j}{2} - \\sum\\limits_{j=1}^m\\alpha_j [ y_j(\\sum\\limits_{i=1}^m\\alpha_i y_i x_i.x_i + b) - 1]\\\\\n",
    "L(\\alpha) = \\frac{\\sum\\limits_{i=1}^m\\sum\\limits_{i=j}^m\\alpha_i y_i x_i\\alpha_j y_j x_j}{2} - \\sum\\limits_{i=1}^m\\sum\\limits_{i=1}^m\\alpha_j y_j(\\alpha_i y_i x_i.x_i) + \\sum\\limits_{j=1}^m\\alpha_j + b\\sum\\limits_{j=1}^m\\alpha_j y_j\\\\\n",
    "  L(\\alpha) = \\sum\\limits_{j=1}^m\\alpha_j -\\frac{\\sum\\limits_{i=1}^m\\sum\\limits_{i=j}^m\\alpha_i y_i x_i\\alpha_j y_j x_j}{2}\\\\\n",
    " \\end{array}\n",
    " $$\n",
    "\n",
    "We have to maixmise $L(\\alpha)$ or minimise -$L(\\alpha)$ subjected to\n",
    "$$ \\begin{array}{ll}\n",
    "\\sum\\limits_{i=1}^m\\alpha_i y_i = 0\\\\\n",
    "and\\\\\n",
    "\\alpha_i \\ge 0,i \\in [1, m] \\\\\n",
    "or \\\\\n",
    "-\\alpha_i \\le 0,i \\in [1, m] \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "According to official documentation of `cvxopt`. If we have a equation of form-\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    " \\mbox{minimize}   & (1/2) x^TPx + q^T x \\\\\n",
    " \\mbox{subject to} & G x \\preceq h \\\\\n",
    "                   & Ax = b.\n",
    " \\end{array}\n",
    "$$\n",
    "\n",
    "we can find `x` by passing these variables in `cvxopt.solvers.qp`  \n",
    "comparing we get\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "P = y_iy_jx_ix_j\\\\\n",
    "q = -1\\\\\n",
    "G = -1\\\\\n",
    "A = y_i\\\\\n",
    "b = 0\\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
