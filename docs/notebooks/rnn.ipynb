{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import  itertools \n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    xt = np.exp(x - np.max(x))\n",
    "    return xt / np.sum(xt)\n",
    "\n",
    "def save_model_parameters_theano(outfile, model):\n",
    "    U, V, W = model.U.get_value(), model.V.get_value(), model.W.get_value()\n",
    "    np.savez(outfile, U=U, V=V, W=W)\n",
    "    print (\"Saved model parameters to %s.\" % outfile)\n",
    "\n",
    "def load_model_parameters_theano(path, model):\n",
    "    npzfile = np.load(path)\n",
    "    U, V, W = npzfile[\"U\"], npzfile[\"V\"], npzfile[\"W\"]\n",
    "    model.hidden_dim = U.shape[0]\n",
    "    model.word_dim = U.shape[1]\n",
    "    model.U.set_value(U)\n",
    "    model.V.set_value(V)\n",
    "    model.W.set_value(W)\n",
    "    print (\"Loaded model parameters from %s. hidden_dim=%d word_dim=%d\" % (path, U.shape[0], U.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/markroxor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Reading CSV file...\n",
      "Parsed 79170 sentences.\n",
      "Found 65441 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'crank' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import itertools\n",
    "\n",
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "print (\"Reading CSV file...\")\n",
    "with open('mldata/reddit-comments-2015-08.csv', 'r') as f:\n",
    "#     reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader = csv.DictReader(f)\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x['body'].lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print (\"Parsed %d sentences.\" % (len(sentences)))\n",
    "    \n",
    "# Tokenize the sentences into words\n",
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "print (\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "print (\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print (\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "print (\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print (\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 51, 27, 16, 10, 858, 54, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 858, 54, 25, 34, 69, 1]\n"
     ]
    }
   ],
   "source": [
    "# Print an training data example\n",
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print (\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  RNNNumpy :\n",
    "    \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    s = np.zeros((T + 1, self.hidden_dim))\n",
    "    s[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "        o[t] = softmax(self.V.dot(s[t]))\n",
    "    return [o, s]\n",
    "\n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    "\n",
    "RNNNumpy.predict = predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 72, 63, 13, 124, 5, 26, 1128, 208, 5, 324, 3, 329, 4, 112, 32, 75, 7, 4745, 4, 8, 84, 52, 9, 7, 3155, 1021, 492, 7535, 8, 133, 48, 3096, 4, 10, 95, 51, 4, 128, 17, 37, 314, 577, 2, 40]\n",
      "(45, 8000)\n",
      "[[0.00012408 0.0001244  0.00012603 ... 0.00012515 0.00012488 0.00012508]\n",
      " [0.00012536 0.00012582 0.00012436 ... 0.00012482 0.00012456 0.00012451]\n",
      " [0.00012387 0.0001252  0.00012474 ... 0.00012559 0.00012588 0.00012551]\n",
      " ...\n",
      " [0.00012471 0.0001243  0.00012524 ... 0.00012475 0.00012522 0.00012623]\n",
      " [0.00012564 0.00012431 0.00012481 ... 0.0001244  0.00012609 0.00012486]\n",
      " [0.00012447 0.00012509 0.00012469 ... 0.00012473 0.00012506 0.00012641]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "\n",
    "print(X_train[10])\n",
    "print (o.shape)\n",
    "print (o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "[0, 72, 63, 13, 124, 5, 26, 1128, 208, 5, 324, 3, 329, 4, 112, 32, 75, 7, 4745, 4, 8, 84, 52, 9, 7, 3155, 1021, 492, 7535, 8, 133, 48, 3096, 4, 10, 95, 51, 4, 128, 17, 37, 314, 577, 2, 40]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[10]))\n",
    "print(X_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "[1284 5221 7653 7430 1013 3562 7366 1874  224 6601 7299 6722 6892 3198\n",
      " 4480 5853 2926  261  489  760 1810 5376 4146  477 7051 5981 1549 3765\n",
      " 2493 1835 1900 4323 2579 5879 4864 5132 6569 2800 2752 6821 4437 7021\n",
      " 3943 6912 3922]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print (predictions.shape)\n",
    "print (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    "\n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 8.987197\n",
      "Actual loss: 8.987406\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print (\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print (\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    "\n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markroxor/Documents/jellAIfish/fromscratchtoml/env/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = model.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print (\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = model.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = model.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is to large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print (\"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
    "                print (\"+h Loss: %f\" % gradplus)\n",
    "                print (\"-h Loss: %f\" % gradminus)\n",
    "                print (\"Estimated_gradient: %f\" % estimated_gradient)\n",
    "                print (\"Backpropagation gradient: %f\" % backprop_gradient)\n",
    "                print (\"Relative Error: %f\" % relative_error)\n",
    "                return \n",
    "            it.iternext()\n",
    "        print (\"Gradient check for parameter %s passed.\" % (pname))\n",
    "\n",
    "RNNNumpy.gradient_check = gradient_check\n",
    "\n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "\n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5  \n",
    "                print (\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 ms ± 5.45 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-27 17:02:50: Loss after num_examples_seen=0 epoch=0: 8.987250\n",
      "2018-06-27 17:02:56: Loss after num_examples_seen=100 epoch=1: 8.975955\n",
      "2018-06-27 17:03:03: Loss after num_examples_seen=200 epoch=2: 8.959699\n",
      "2018-06-27 17:03:09: Loss after num_examples_seen=300 epoch=3: 8.929399\n",
      "2018-06-27 17:03:16: Loss after num_examples_seen=400 epoch=4: 8.845521\n",
      "2018-06-27 17:03:23: Loss after num_examples_seen=500 epoch=5: 6.778843\n",
      "2018-06-27 17:03:30: Loss after num_examples_seen=600 epoch=6: 6.263977\n",
      "2018-06-27 17:03:36: Loss after num_examples_seen=700 epoch=7: 5.999944\n",
      "2018-06-27 17:03:43: Loss after num_examples_seen=800 epoch=8: 5.829725\n",
      "2018-06-27 17:03:49: Loss after num_examples_seen=900 epoch=9: 5.714151\n",
      "2018-06-27 17:03:56: Loss after num_examples_seen=1000 epoch=10: 5.630104\n",
      "2018-06-27 17:04:02: Loss after num_examples_seen=1100 epoch=11: 5.564262\n",
      "2018-06-27 17:04:10: Loss after num_examples_seen=1200 epoch=12: 5.510284\n",
      "2018-06-27 17:04:17: Loss after num_examples_seen=1300 epoch=13: 5.464627\n",
      "2018-06-27 17:04:24: Loss after num_examples_seen=1400 epoch=14: 5.426093\n",
      "2018-06-27 17:04:31: Loss after num_examples_seen=1500 epoch=15: 5.394061\n",
      "2018-06-27 17:04:37: Loss after num_examples_seen=1600 epoch=16: 5.367787\n",
      "2018-06-27 17:04:44: Loss after num_examples_seen=1700 epoch=17: 5.345732\n",
      "2018-06-27 17:04:51: Loss after num_examples_seen=1800 epoch=18: 5.326110\n",
      "2018-06-27 17:04:57: Loss after num_examples_seen=1900 epoch=19: 5.307878\n",
      "2018-06-27 17:05:04: Loss after num_examples_seen=2000 epoch=20: 5.292367\n",
      "2018-06-27 17:05:11: Loss after num_examples_seen=2100 epoch=21: 5.279341\n",
      "2018-06-27 17:05:18: Loss after num_examples_seen=2200 epoch=22: 5.268104\n",
      "2018-06-27 17:05:24: Loss after num_examples_seen=2300 epoch=23: 5.251886\n",
      "2018-06-27 17:05:31: Loss after num_examples_seen=2400 epoch=24: 5.233599\n",
      "2018-06-27 17:05:38: Loss after num_examples_seen=2500 epoch=25: 5.244874\n",
      "Setting learning rate to 0.002500\n",
      "2018-06-27 17:05:45: Loss after num_examples_seen=2600 epoch=26: 5.156438\n",
      "2018-06-27 17:05:51: Loss after num_examples_seen=2700 epoch=27: 5.140808\n",
      "2018-06-27 17:05:58: Loss after num_examples_seen=2800 epoch=28: 5.128698\n",
      "2018-06-27 17:06:06: Loss after num_examples_seen=2900 epoch=29: 5.110450\n",
      "2018-06-27 17:06:13: Loss after num_examples_seen=3000 epoch=30: 5.092315\n",
      "2018-06-27 17:06:19: Loss after num_examples_seen=3100 epoch=31: 5.076784\n",
      "2018-06-27 17:06:26: Loss after num_examples_seen=3200 epoch=32: 5.061971\n",
      "2018-06-27 17:06:32: Loss after num_examples_seen=3300 epoch=33: 5.046515\n",
      "2018-06-27 17:06:39: Loss after num_examples_seen=3400 epoch=34: 5.033430\n",
      "2018-06-27 17:06:46: Loss after num_examples_seen=3500 epoch=35: 5.023451\n",
      "2018-06-27 17:06:53: Loss after num_examples_seen=3600 epoch=36: 5.014629\n",
      "2018-06-27 17:07:00: Loss after num_examples_seen=3700 epoch=37: 5.012789\n",
      "2018-06-27 17:07:08: Loss after num_examples_seen=3800 epoch=38: 5.004565\n",
      "2018-06-27 17:07:14: Loss after num_examples_seen=3900 epoch=39: 4.995525\n",
      "2018-06-27 17:07:21: Loss after num_examples_seen=4000 epoch=40: 4.985999\n",
      "2018-06-27 17:07:27: Loss after num_examples_seen=4100 epoch=41: 4.976174\n",
      "2018-06-27 17:07:34: Loss after num_examples_seen=4200 epoch=42: 4.966145\n",
      "2018-06-27 17:07:41: Loss after num_examples_seen=4300 epoch=43: 4.952130\n",
      "2018-06-27 17:07:47: Loss after num_examples_seen=4400 epoch=44: 4.944748\n",
      "2018-06-27 17:07:54: Loss after num_examples_seen=4500 epoch=45: 4.930049\n",
      "2018-06-27 17:08:02: Loss after num_examples_seen=4600 epoch=46: 4.917075\n",
      "2018-06-27 17:08:08: Loss after num_examples_seen=4700 epoch=47: 4.910832\n",
      "2018-06-27 17:08:15: Loss after num_examples_seen=4800 epoch=48: 4.891316\n",
      "2018-06-27 17:08:22: Loss after num_examples_seen=4900 epoch=49: 4.886811\n",
      "2018-06-27 17:08:29: Loss after num_examples_seen=5000 epoch=50: 4.881796\n",
      "2018-06-27 17:08:36: Loss after num_examples_seen=5100 epoch=51: 4.872483\n",
      "2018-06-27 17:08:43: Loss after num_examples_seen=5200 epoch=52: 4.880795\n",
      "Setting learning rate to 0.001250\n",
      "2018-06-27 17:08:50: Loss after num_examples_seen=5300 epoch=53: 4.871631\n",
      "2018-06-27 17:08:56: Loss after num_examples_seen=5400 epoch=54: 4.873787\n",
      "Setting learning rate to 0.000625\n",
      "2018-06-27 17:09:03: Loss after num_examples_seen=5500 epoch=55: 4.835197\n",
      "2018-06-27 17:09:10: Loss after num_examples_seen=5600 epoch=56: 4.835038\n",
      "2018-06-27 17:09:18: Loss after num_examples_seen=5700 epoch=57: 4.823841\n",
      "2018-06-27 17:09:25: Loss after num_examples_seen=5800 epoch=58: 4.823400\n",
      "2018-06-27 17:09:32: Loss after num_examples_seen=5900 epoch=59: 4.821211\n",
      "2018-06-27 17:09:39: Loss after num_examples_seen=6000 epoch=60: 4.816573\n",
      "2018-06-27 17:09:45: Loss after num_examples_seen=6100 epoch=61: 4.821281\n",
      "Setting learning rate to 0.000313\n",
      "2018-06-27 17:09:53: Loss after num_examples_seen=6200 epoch=62: 4.806346\n",
      "2018-06-27 17:10:00: Loss after num_examples_seen=6300 epoch=63: 4.806554\n",
      "Setting learning rate to 0.000156\n",
      "2018-06-27 17:10:06: Loss after num_examples_seen=6400 epoch=64: 4.796908\n",
      "2018-06-27 17:10:14: Loss after num_examples_seen=6500 epoch=65: 4.794538\n",
      "2018-06-27 17:10:21: Loss after num_examples_seen=6600 epoch=66: 4.791586\n",
      "2018-06-27 17:10:28: Loss after num_examples_seen=6700 epoch=67: 4.789106\n",
      "2018-06-27 17:10:36: Loss after num_examples_seen=6800 epoch=68: 4.786487\n",
      "2018-06-27 17:10:45: Loss after num_examples_seen=6900 epoch=69: 4.783117\n",
      "2018-06-27 17:10:52: Loss after num_examples_seen=7000 epoch=70: 4.778738\n",
      "2018-06-27 17:11:00: Loss after num_examples_seen=7100 epoch=71: 4.775494\n",
      "2018-06-27 17:11:09: Loss after num_examples_seen=7200 epoch=72: 4.774027\n",
      "2018-06-27 17:11:16: Loss after num_examples_seen=7300 epoch=73: 4.772425\n",
      "2018-06-27 17:11:25: Loss after num_examples_seen=7400 epoch=74: 4.771745\n",
      "2018-06-27 17:11:33: Loss after num_examples_seen=7500 epoch=75: 4.771066\n",
      "2018-06-27 17:11:42: Loss after num_examples_seen=7600 epoch=76: 4.770111\n",
      "2018-06-27 17:11:49: Loss after num_examples_seen=7700 epoch=77: 4.768930\n",
      "2018-06-27 17:11:57: Loss after num_examples_seen=7800 epoch=78: 4.766999\n",
      "2018-06-27 17:12:04: Loss after num_examples_seen=7900 epoch=79: 4.765281\n",
      "2018-06-27 17:12:14: Loss after num_examples_seen=8000 epoch=80: 4.763813\n",
      "2018-06-27 17:12:21: Loss after num_examples_seen=8100 epoch=81: 4.762856\n",
      "2018-06-27 17:12:28: Loss after num_examples_seen=8200 epoch=82: 4.762447\n",
      "2018-06-27 17:12:36: Loss after num_examples_seen=8300 epoch=83: 4.761611\n",
      "2018-06-27 17:12:44: Loss after num_examples_seen=8400 epoch=84: 4.760875\n",
      "2018-06-27 17:12:53: Loss after num_examples_seen=8500 epoch=85: 4.760061\n",
      "2018-06-27 17:13:00: Loss after num_examples_seen=8600 epoch=86: 4.759303\n",
      "2018-06-27 17:13:08: Loss after num_examples_seen=8700 epoch=87: 4.758475\n",
      "2018-06-27 17:13:15: Loss after num_examples_seen=8800 epoch=88: 4.757210\n",
      "2018-06-27 17:13:23: Loss after num_examples_seen=8900 epoch=89: 4.756302\n",
      "2018-06-27 17:13:30: Loss after num_examples_seen=9000 epoch=90: 4.755146\n",
      "2018-06-27 17:13:38: Loss after num_examples_seen=9100 epoch=91: 4.754425\n",
      "2018-06-27 17:13:46: Loss after num_examples_seen=9200 epoch=92: 4.752965\n",
      "2018-06-27 17:13:53: Loss after num_examples_seen=9300 epoch=93: 4.751777\n",
      "2018-06-27 17:14:01: Loss after num_examples_seen=9400 epoch=94: 4.750788\n",
      "2018-06-27 17:14:07: Loss after num_examples_seen=9500 epoch=95: 4.749935\n",
      "2018-06-27 17:14:14: Loss after num_examples_seen=9600 epoch=96: 4.749178\n",
      "2018-06-27 17:14:21: Loss after num_examples_seen=9700 epoch=97: 4.748526\n",
      "2018-06-27 17:14:28: Loss after num_examples_seen=9800 epoch=98: 4.747912\n",
      "2018-06-27 17:14:35: Loss after num_examples_seen=9900 epoch=99: 4.747140\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch=100, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 50, 17, 154, 83, 7, 3413, 561, 4578, 4, 172, 453, 9, 5417, 13, 813, 473, 2] [50, 17, 154, 83, 7, 3413, 561, 4578, 4, 172, 453, 9, 5417, 13, 813, 473, 2, 1]\n",
      "[0, 92, 5688, 7999, 17, 7999, 13, 7999, 5, 3, 2131, 2] [92, 5688, 7999, 17, 7999, 13, 7999, 5, 3, 2131, 2, 1]\n",
      "[0, 25, 13, 4751, 33, 11, 13, 328, 515, 9, 7999, 2] [25, 13, 4751, 33, 11, 13, 328, 515, 9, 7999, 2, 1]\n",
      "[0, 4751, 7999, 4, 4751, 4957, 2] [4751, 7999, 4, 4751, 4957, 2, 1]\n",
      "[0, 25, 269, 87, 16, 1776, 7999, 3, 145, 111, 2] [25, 269, 87, 16, 1776, 7999, 3, 145, 111, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(X_train[100:105], y_train[100:105]):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play it not the have out notifications have and charles n't they all expanded the adventures to the ’ or http observe who . the the lean of it where there and to on the editor for it it\n",
      "kickstarter valuable skyrim prison or if zone joe going so the .\n",
      "finale the tears the and n't with .\n",
      "detection all is to but new use uncle have on ’ is weapon of his and require .\n",
      "point fighting shaped been serve fuel to than the the sound i of cause some the it consumers have did the the 'm is the and 2016 bikes reports hope reports the /r/askscience mediocre brisbane n't the be when would walls the `` amazingly .\n",
      "eventually it combo like the we units our to aus to entry the them rick was annoying the\n",
      "jacket have a not the number '' for puts do for\n",
      "studied rumors there after would n't ) consequence them in overwhelmed or on not 300 than be of the include i obligation the\n",
      "pen thursday not on and oblivious judge going the guitar & get had still ; them mod and crap a are presented the everywhere the film your mods our deserve\n",
      "tires authors but of corsair if .\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        next_word_probs, _ = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "#         print(index_to_word[sampled_word], sampled_word)\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    " \n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    " \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print (\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the just wo a trigger her and rebates it .\n",
      "it it is trapped require of vote sellers enter resolution , like year yards 3 young or n't .\n",
      "ppr gt toss i making be when they do made at be on probably anyone no to otherwise that my .\n",
      "no never years young utilize players find the and run gun , like even , and my had in but did logic she minimal ability did but i the n't to been year\n",
      "fucking i out you back do this bloody 3-4 quad-core the would `` really if as waking 's : the whether to if ppr side .\n",
      "reform gt used you have we the get for n't de of still other kicked more supporting of be rolls with i been defense but turn other , i nearly 'll that to solid the of a close be check this 300 the out .\n",
      "bonuses person you ) ie anyone like be it and i and be them i regard i remember player other must but require n't mandatory n't not fact not ( existing who `` form , .\n",
      "outside having ; take an also `` i qb in playing .\n",
      "i 's judge yards for it before '' 's restricted .\n",
      "a he the , like in attacking front .\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence(model):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [word_to_index[sentence_start_token]]\n",
    "    # Repeat until we get an end token\n",
    "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
    "        next_word_probs = model.forward_propagation(new_sentence)\n",
    "        next_word_probs, _ = model.forward_propagation(new_sentence)\n",
    "        sampled_word = word_to_index[unknown_token]\n",
    "#         print(index_to_word[sampled_word], sampled_word)\n",
    "        # We don't want to sample unknown words\n",
    "        while sampled_word == word_to_index[unknown_token]:\n",
    "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
    "            sampled_word = np.argmax(samples)\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return sentence_str\n",
    " \n",
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    " \n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sentence(model)\n",
    "    print (\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fs2ml",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
